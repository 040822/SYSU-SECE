{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp30SB4bxeQb"
      },
      "source": [
        "# **Homework 12 - Reinforcement Learning**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXsnCWPtWSNk"
      },
      "source": [
        "## Preliminary work\n",
        "## 准备工作\n",
        "\n",
        "First, we need to install all necessary packages.\n",
        "One of them, gym, builded by OpenAI, is a toolkit for developing Reinforcement Learning algorithm. Other packages are for visualization in colab.  \n",
        "  \n",
        "首先，我们需要安装一堆包。其中gym包是用来实现RL的算法的，其他都是用来可视化的。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e2bScpnkVbv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dea60742-4a02-414e-fc63-3557a0f52631"
      },
      "source": [
        "!apt update\n",
        "!apt install python-opengl xvfb -y\n",
        "!pip install gym[box2d]==0.18.3 pyvirtualdisplay tqdm numpy==1.19.5 torch==1.8.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.39)] [Connecting to security.ub\u001b[0m\r                                                                               \rHit:2 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.39)] [Connecting to security.ub\u001b[0m\r                                                                               \rHit:3 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.39)] [Connecting to security.ub\u001b[0m\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.39)] [Connecting to security.ub\u001b[0m\r                                                                               \rHit:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connected to cloud.r-project.or\u001b[0m\r                                                                               \rHit:6 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "\u001b[33m\r                                                                               \r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\u001b[0m\r                                                                    \rHit:7 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "\u001b[33m\r                                                                    \r0% [Waiting for headers] [Waiting for headers]\u001b[0m\r                                              \rHit:8 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:9 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "27 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-2build1).\n",
            "xvfb is already the newest version (2:1.20.13-1ubuntu1~20.04.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 27 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym[box2d]==0.18.3\n",
            "  Using cached gym-0.18.3.tar.gz (1.6 MB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_-i3cdoYsks"
      },
      "source": [
        "\n",
        "Next, set up virtual display，and import all necessaary packages.\n",
        "  \n",
        "import一堆包，并设置可视化页面"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl2nREINDLiw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "4d9e71cf-5a3b-4058-a36f-636045e5b9bd"
      },
      "source": [
        "%%capture\n",
        "from pyvirtualdisplay import Display\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "%%capture\n",
        "import gym\n",
        "import random\n",
        "\n",
        "\n",
        "#https://www.gymlibrary.dev/content/basic_usage/  gym库"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-1c2e31dd6153>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyvirtualdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvirtual_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m900\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvirtual_display\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyvirtualdisplay'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaEJ8BUCpN9P"
      },
      "source": [
        "# Warning ! Do not revise random seed !!!\n",
        "# Your submission on JudgeBoi will not reproduce your result !!!\n",
        "Make your HW result to be reproducible.\n",
        "\n",
        "  \n",
        "助教不允许我们用随机种子\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV9i8i2YkRbO"
      },
      "source": [
        "#固定种子\n",
        "\n",
        "seed = 543 # Do not change this\n",
        "def fix(env, seed):\n",
        "  env.seed(seed)\n",
        "  env.action_space.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "  torch.set_deterministic(True)\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He0XDx6bzjgC"
      },
      "source": [
        "Last, call gym and build an [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) environment.\n",
        "  \n",
        "使用gym，建立“Lunar Lander”环境"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_4-xJcbBt09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "ca1d9aaf-5fde-4b21-d647-32719271cdcb"
      },
      "source": [
        "%%capture\n",
        "import gym\n",
        "import random\n",
        "env = gym.make('LunarLander-v2')\n",
        "fix(env, seed) # fix the environment Do not revise this !!!\n",
        "\n",
        "# 建立一个环境"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DependencyNotInstalled",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mBox2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     from Box2D.b2 import (\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Box2D'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-634c21dd0ddf>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LunarLander-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fix the environment Do not revise this !!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, new_step_api, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0;31m# Assume it's a string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0menv_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"render_mode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \"\"\"\n\u001b[1;32m     61\u001b[0m     \u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbipedal_walker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBipedalWalker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBipedalWalkerHardcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcar_racing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCarRacing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlunar_lander\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLunarLander\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLunarLanderContinuous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mDependencyNotInstalled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"box2D is not installed, run `pip install gym[box2d]`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDependencyNotInstalled\u001b[0m: box2D is not installed, run `pip install gym[box2d]`"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrkVvTrvWZ5H"
      },
      "source": [
        "## What Lunar Lander？\n",
        "\n",
        "“LunarLander-v2”is to simulate the situation when the craft lands on the surface of the moon.  \n",
        "\n",
        "\n",
        "This task is to enable the craft to land \"safely\" at the pad between the two yellow flags.\n",
        "> Landing pad is always at coordinates (0,0).\n",
        "> Coordinates are the first two numbers in state vector.\n",
        "\n",
        "![](https://gym.openai.com/assets/docs/aeloop-138c89d44114492fd02822303e6b4b07213010bb14ca5856d2d49d6b62d88e53.svg)\n",
        "\n",
        "\"LunarLander-v2\" actually includes \"Agent\" and \"Environment\". \n",
        "\n",
        "In this homework, we will utilize the function `step()` to control the action of \"Agent\". \n",
        "\n",
        "Then `step()` will return the observation/state and reward given by the \"Environment\".  \n",
        "\n",
        "\n",
        "  \n",
        "这玩意是用来模拟飞行器降落在月球表面的情况的东西。  \n",
        "其目标是让飞行器安全地在两个黄旗子之间的垫子上着陆\n",
        "> 着陆垫始终在坐标（0，0）处  \n",
        "> 坐标是state vector（状态向量）的前两个数字？\n",
        "  \n",
        "  \n",
        "\"LunarLander-v2\"已经集成了“Agent”和“Environment”  \n",
        "在本次作业中，我们要使用step（）函数去控制“Agent”的行动  \n",
        "然后step（）会返回observation和环境的reward。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 官方文档中的信息\n",
        "[文档](https://www.gymlibrary.dev/environments/box2d/lunar_lander/)\n",
        "\n",
        "##Actions：\n",
        "四种\n",
        "- 0代表agent啥也不做   \n",
        "- 1、2、3代表左边、向下、右边引擎开火（喷射）\n",
        "  \n",
        "##Observation space: \n",
        "八维向量，例如[1.5 1.5 5. 5. 3.14 5. 1. 1. ]  \n",
        "分别代表[坐标x，坐标y，x方向分速度，y方向分速度，角度，角速度，bool，bool]  \n",
        "两个bool表示飞行器的两只脚是否着陆。\n",
        "\n",
        "##Reward:\n",
        "登陆垫始终在（0，0）。坐标是状态向量（那个八维向量）的前两维。 从屏幕顶端到达登陆垫并保持速度为0的reward是大概100-140pt。如果登陆器从登陆垫上移动出去了，reward就没了。当登陆器被摧毁了（-100pt）或者成功着陆并休息后（+100pt），一个episode结束。登陆器的每条腿接触地面会+10pt。 主引擎点火为每帧-3分。 解决了加200分\n",
        "\n",
        "-在黄旗之间着陆    +100-140pt\n",
        "-着陆但移动出去了  上边的pt没了\n",
        "-坠毁           -100pt\n",
        "-主引擎点火(向下喷射) 每帧-0.3pt\n",
        "-左右引擎点火.    每帧-0.03pt\n",
        "-登陆器最终静止(rest)  再+100pt\n",
        "-每条腿接触地面.      +10pt\n",
        "\n",
        "-解决了（solved）（？）  +200pt\n",
        "\n",
        "  \n",
        "## Starting State\n",
        "The lander starts at the top center of the viewport with a random initial force applied to its center of mass.  \n",
        "着陆器\n",
        "\n",
        "## Episode Termination\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "f9sRvDdL4mln"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIbp82sljvAt"
      },
      "source": [
        "# Observation / State\n",
        "\n",
        "First, we can take a look at what an Observation / State looks like.  \n",
        "Observation长下边这样"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "rsXZra3N9R5T",
        "outputId": "195232a3-f9f2-46f3-c6bb-5e3e61030fd4"
      },
      "source": [
        "print(env.observation_space)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-687cbce6b1ed>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezdfoThbAQ49"
      },
      "source": [
        "\n",
        "`Box(8,)`means that observation is an 8-dim vector\n",
        "### Action\n",
        "\n",
        "Actions can be taken by looks like  \n",
        "Action长下边这样"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1k4dIrBAaKi"
      },
      "source": [
        "print(env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dejXT6PHBrPn"
      },
      "source": [
        "`Discrete(4)` implies that there are four kinds of actions can be taken by agent.\n",
        "- 0 implies the agent will not take any actions\n",
        "- 2 implies the agent will accelerate downward\n",
        "- 1, 3 implies the agent will accelerate left and right\n",
        "\n",
        "Next, we will try to make the agent interact with the environment. \n",
        "Before taking any actions, we recommend to call `reset()` function to reset the environment. Also, this function will return the initial state of the environment.\n",
        "  \n",
        "这表明一共有四种行动。  \n",
        "- 0代表agent啥也不做  \n",
        "- 2代表它向下加速  \n",
        "- 1、3代表它左移和右移  \n",
        "  \n",
        "下边我们将会尝试让agent与环境互动  \n",
        "在agent采取行动之前，我们推荐用reset（）去reset环境，这个函数也会返回环境的初始状态"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi4OmrmZgnWA"
      },
      "source": [
        "initial_state = env.reset()\n",
        "print(initial_state)\n",
        "\n",
        "#初始环境长下边这样，和Box一样，也是八维向量"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBx0mEqqgxJ9"
      },
      "source": [
        "Then, we try to get a random action from the agent's action space.  \n",
        "然后，我们将试着随机得到一种行动"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxkOEXRKgizt"
      },
      "source": [
        "random_action = env.action_space.sample()\n",
        "print(random_action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mns-bO01g0-J"
      },
      "source": [
        "More, we can utilize `step()` to make agent act according to the randomly-selected `random_action`.\n",
        "The `step()` function will return four values:\n",
        "- observation / state\n",
        "- reward\n",
        "- done (True/ False)\n",
        "- Other information  \n",
        "\n",
        "下边，我们可以用step（）去使agent按照上边随机抽取的random_action进行行动  \n",
        "step（）将会返回四个值：  \n",
        "- observation / state\n",
        "- reward\n",
        "- done (True/ False)\n",
        "- Other information  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_WViSxGgIk9"
      },
      "source": [
        "observation, reward, done, info = env.step(random_action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK7r126kuCNp"
      },
      "source": [
        "print(done)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKdS8vOihxhc"
      },
      "source": [
        "### Reward\n",
        "\n",
        "\n",
        "> Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points.   \n",
        "\n",
        "登陆垫始终在（0，0）。坐标是状态向量（那个八维向量）的前两维。 从屏幕顶端到达登陆垫并保持速度为0的reward是大概100-140pt。如果登陆器从登陆垫上移动出去了，reward就没了。当登陆器被摧毁了（-100pt）或者成功着陆并休息后（+100pt），一个episode结束。登陆器的每条腿接触地面会+10pt。 主引擎点火为每帧-3分。 解决了（？）加200分"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxQNs77hi0_7"
      },
      "source": [
        "print(reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhqp6D-XgHpe"
      },
      "source": [
        "# Random Agent\n",
        "In the end, before we start training, we can see whether a random agent can successfully land the moon or not.  \n",
        "在开始训练之前，我们可以看看一个随机生成的agent是否能够成功地着陆"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3G0bxoccelv"
      },
      "source": [
        "env.reset()\n",
        "\n",
        "img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    #Randomly sample an element of this space.\n",
        "    #随机抽取动作空间里的一个元素\n",
        "\n",
        "    observation, reward, done, _ = env.step(action)\n",
        "    \n",
        "'''\n",
        "下边，我们可以用step（）去使agent按照上边随机抽取的random_action进行行动  \n",
        "step（）将会返回四个值：  \n",
        "- observation / state\n",
        "- reward\n",
        "- done (True/ False)\n",
        "- Other information  \n",
        "'''\n",
        "\n",
        "\n",
        "    img.set_data(env.render(mode='rgb_array'))\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5paWqo7tWL2"
      },
      "source": [
        "## Policy Gradient\n",
        "Now, we can build a simple policy network. The network will return one of action in the action space.  \n",
        "现在，我们将创建一个简单的policy network，它将返回一项动作。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8tdmeD-tZew"
      },
      "source": [
        "class PolicyGradientNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(8, 16)\n",
        "        self.fc2 = nn.Linear(16, 16)\n",
        "        self.fc3 = nn.Linear(16, 4)\n",
        "\n",
        "        #三层线性层 8=>16=>16=>4\n",
        "\n",
        "    def forward(self, state):\n",
        "        hid = torch.tanh(self.fc1(state))\n",
        "        hid = torch.tanh(self.fc2(hid))\n",
        "        return F.softmax(self.fc3(hid), dim=-1)\n",
        "\n",
        "        #两次tanh一次softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynbqJrhIFTC3"
      },
      "source": [
        "Then, we need to build a simple agent. The agent will acts according to the output of the policy network above. There are a few things can be done by agent:\n",
        "- `learn()`：update the policy network from log probabilities and rewards.\n",
        "- `sample()`：After receiving observation from the environment, utilize policy network to tell which action to take. The return values of this function includes action and log probabilities.  \n",
        "  \n",
        "下边，我们需要创建一个简单的agent。它能够做以下事情：  \n",
        "- learn（）：使用log probabilities and rewards来更新policy network  \n",
        "- sample（）：在接收observation后，使用policy network去告诉agent下边需要采取哪种行动，这个函数的返回值包括行动和log probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZo-IxJx286z"
      },
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "class PolicyGradientAgent():\n",
        "    \n",
        "    def __init__(self, network):\n",
        "        self.network = network\n",
        "        self.optimizer = optim.SGD(self.network.parameters(), lr=0.001)\n",
        "        #优化算法：SGD\n",
        "        \n",
        "    def forward(self, state):\n",
        "        return self.network(state)\n",
        "    def learn(self, log_probs, rewards):\n",
        "        loss = (-log_probs * rewards).sum() \n",
        "\n",
        "        # You don't need to revise this to pass simple baseline (but you can)\n",
        "        #你无需修改loss算法就可以通过simple baseline，但是你可以改\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def sample(self, state):\n",
        "        action_prob = self.network(torch.FloatTensor(state))\n",
        "        action_dist = Categorical(action_prob)\n",
        "        action = action_dist.sample()\n",
        "        log_prob = action_dist.log_prob(action)\n",
        "        return action.item(), log_prob\n",
        "        #返回action种类，和log probability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehPlnTKyRZf9"
      },
      "source": [
        "Lastly, build a network and agent to start training.  \n",
        "最后，创建（实例化）一个network和agent，然后开始训练！"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfJIvML-RYjL"
      },
      "source": [
        "network = PolicyGradientNetwork()\n",
        "agent = PolicyGradientAgent(network)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouv23glgf5Qt"
      },
      "source": [
        "## Training Agent\n",
        "\n",
        "Now let's start to train our agent.\n",
        "Through taking all the interactions between agent and environment as training data, the policy network can learn from all these attempts,  \n",
        "  \n",
        "下边我们开始训练agent了 \n",
        "通过把agent和环境间的所有互动当作训练数据，policy network可以从所有的attempts中学习\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg5rxBBaf38_"
      },
      "source": [
        "agent.network.train()  \n",
        "# Switch network into training mode \n",
        "# 转换为train模式 \n",
        "EPISODE_PER_BATCH = 5  \n",
        "# update the  agent every 5 episode\n",
        "# 每5个episode更新一次agent\n",
        "\n",
        "NUM_BATCH = 500        \n",
        "# totally update the agent for 400 time（？）\n",
        "# 一共更新400次\n",
        "\n",
        "avg_total_rewards, avg_final_rewards = [], []\n",
        "\n",
        "prg_bar = tqdm(range(NUM_BATCH))\n",
        "for batch in prg_bar:\n",
        "\n",
        "    log_probs, rewards = [], []\n",
        "    total_rewards, final_rewards = [], []\n",
        "\n",
        "    # collect trajectory\n",
        "    for episode in range(EPISODE_PER_BATCH):\n",
        "        \n",
        "        state = env.reset()\n",
        "        total_reward, total_step = 0, 0\n",
        "        seq_rewards = []\n",
        "        while True:\n",
        "\n",
        "            action, log_prob = agent.sample(state) # at, log(at|st)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            log_probs.append(log_prob) # [log(a1|s1), log(a2|s2), ...., log(at|st)]\n",
        "            # seq_rewards.append(reward)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            total_step += 1\n",
        "            rewards.append(reward) # change here\n",
        "            # ! IMPORTANT !\n",
        "            # Current reward implementation: immediate reward,  given action_list : a1, a2, a3 ......\n",
        "            #                                                         rewards :     r1, r2 ,r3 ......\n",
        "            # medium：change \"rewards\" to accumulative decaying reward, given action_list : a1,                           a2,                           a3, ......\n",
        "            #                                                           rewards :           r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,  r3+0.99*r4+0.99^2*r5+ ......\n",
        "            # boss : implement Actor-Critic\n",
        "            if done:\n",
        "                final_rewards.append(reward)\n",
        "                total_rewards.append(total_reward)\n",
        "                \n",
        "                break\n",
        "\n",
        "    print(f\"rewards looks like \", np.shape(rewards))  \n",
        "    print(f\"log_probs looks like \", np.shape(log_probs))     \n",
        "    # record training process\n",
        "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
        "    avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
        "    avg_total_rewards.append(avg_total_reward)\n",
        "    avg_final_rewards.append(avg_final_reward)\n",
        "    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
        "\n",
        "    # update agent\n",
        "    # rewards = np.concatenate(rewards, axis=0)\n",
        "    rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # normalize the reward \n",
        "    agent.learn(torch.stack(log_probs), torch.from_numpy(rewards))\n",
        "    print(\"logs prob looks like \", torch.stack(log_probs).size())\n",
        "    print(\"torch.from_numpy(rewards) looks like \", torch.from_numpy(rewards).size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNb_tuFYhKVK"
      },
      "source": [
        "### Training Result\n",
        "During the training process, we recorded `avg_total_reward`, which represents the average total reward of episodes before updating the policy network.\n",
        "\n",
        "Theoretically, if the agent becomes better, the `avg_total_reward` will increase.\n",
        "The visualization of the training process is shown below:  \n",
        "  \n",
        "\n",
        "###训练结果  \n",
        "通过训练过程，我们记录了“avg_total_reward”,它表示在更新policy network之前的平均的total reward  \n",
        "理论上，如果agent变好了，那么avg_total_reward也会增加。  \n",
        "训练过程的可视化见下"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZYOI8H10SHN"
      },
      "source": [
        "plt.plot(avg_total_rewards)\n",
        "plt.title(\"Total Rewards\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV5jj4dThz0Y"
      },
      "source": [
        "In addition, `avg_final_reward` represents average final rewards of episodes. To be specific, final rewards is the last reward received in one episode, indicating whether the craft lands successfully or not.  \n",
        "  \n",
        "另外，avg_final_reward代表了每个episode的平均final reward  \n",
        "特别的，final reward是一个episode中最后一次获得的reward，表明飞行器是否成功着陆\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txDZ5vlGWz5w"
      },
      "source": [
        "plt.plot(avg_final_rewards)\n",
        "plt.title(\"Final Rewards\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2HaGRVEYGQS"
      },
      "source": [
        "## Testing\n",
        "The testing result will be the average reward of 5 testing  \n",
        "测试结果是五次测试的reward的平均值"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yFuUKKRYH73"
      },
      "source": [
        "fix(env, seed)\n",
        "agent.network.eval()  # set the network into evaluation mode\n",
        "NUM_OF_TEST = 5 # Do not revise this !!!\n",
        "test_total_reward = []\n",
        "action_list = []\n",
        "for i in range(NUM_OF_TEST):\n",
        "  actions = []\n",
        "  state = env.reset()\n",
        "\n",
        "  img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "  total_reward = 0\n",
        "\n",
        "  done = False\n",
        "  while not done:\n",
        "      action, _ = agent.sample(state)\n",
        "      actions.append(action)\n",
        "      state, reward, done, _ = env.step(action)\n",
        "\n",
        "      total_reward += reward\n",
        "\n",
        "      img.set_data(env.render(mode='rgb_array'))\n",
        "      display.display(plt.gcf())\n",
        "      display.clear_output(wait=True)\n",
        "      \n",
        "  print(total_reward)\n",
        "  test_total_reward.append(total_reward)\n",
        "\n",
        "  action_list.append(actions) # save the result of testing \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aex7mcKr0J01"
      },
      "source": [
        "print(np.mean(test_total_reward))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leyebGYRpqsF"
      },
      "source": [
        "Action list  \n",
        "动作列表"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGAH4YWDpp4u"
      },
      "source": [
        "print(\"Action list looks like \", action_list)\n",
        "print(\"Action list's shape looks like \", np.shape(action_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNkmwucrHMen"
      },
      "source": [
        "Analysis of actions taken by agent  \n",
        "agent的动作分析"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHdAItjj1nxw"
      },
      "source": [
        "distribution = {}\n",
        "for actions in action_list:\n",
        "  for action in actions:\n",
        "    if action not in distribution.keys():\n",
        "      distribution[action] = 1\n",
        "    else:\n",
        "      distribution[action] += 1\n",
        "print(distribution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ricE0schY75M"
      },
      "source": [
        "Saving the result of Model Testing  \n",
        "保存测试的结果\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZsMkGmIY42b"
      },
      "source": [
        "PATH = \"Action_List.npy\" # Can be modified into the name or path you want\n",
        "np.save(PATH ,np.array(action_list)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asK7WfbkaLjt"
      },
      "source": [
        "### This is the file you need to submit !!!\n",
        "Download the testing result to your device  \n",
        "需要提交的文件\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-CqyhHzaWAL"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seT4NUmWmAZ1"
      },
      "source": [
        "# Server \n",
        "The code below simulate the environment on the judge server. Can be used for testing.\n",
        "  \n",
        "下边的代码模拟了judge系统上的环境，可以用来测试"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U69c-YTxaw6b"
      },
      "source": [
        "action_list = np.load(PATH,allow_pickle=True) # The action list you upload\n",
        "seed = 543 # Do not revise this\n",
        "fix(env, seed)\n",
        "\n",
        "agent.network.eval()  # set network to evaluation mode\n",
        "\n",
        "test_total_reward = []\n",
        "if len(action_list) != 5:\n",
        "  print(\"Wrong format of file !!!\")\n",
        "  exit(0)\n",
        "for actions in action_list:\n",
        "  state = env.reset()\n",
        "  img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "  total_reward = 0\n",
        "\n",
        "  done = False\n",
        "\n",
        "  for action in actions:\n",
        "  \n",
        "      state, reward, done, _ = env.step(action)\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "  print(f\"Your reward is : %.2f\"%total_reward)\n",
        "  test_total_reward.append(total_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjFBWwQP1hVe"
      },
      "source": [
        "# Your score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpJpZz3Wbm0X"
      },
      "source": [
        "print(f\"Your final reward is : %.2f\"%np.mean(test_total_reward))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUBtYXG2eaqf"
      },
      "source": [
        "## Reference\n",
        "\n",
        "Below are some useful tips for you to get high score.\n",
        "\n",
        "- [DRL Lecture 1: Policy Gradient (Review)](https://youtu.be/z95ZYgPgXOY)\n",
        "- [ML Lecture 23-3: Reinforcement Learning (including Q-learning) start at 30:00](https://youtu.be/2-JNBzCq77c?t=1800)\n",
        "- [Lecture 7: Policy Gradient, David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf)\n"
      ]
    }
  ]
}